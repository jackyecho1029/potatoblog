#!/usr/bin/env python3
"""
X Signal Daily Briefing - Python Version
ä½¿ç”¨ Nitter å…¬å¼€ RSS é•œåƒæŠ“å– X (Twitter) å†…å®¹ï¼Œæ— éœ€ API Key
æŒ‰æ¿å—åˆ†ç±»ï¼Œæ¯æ¿å—ç­›é€‰ 5-10 æ¡é«˜ä»·å€¼å†…å®¹ï¼Œç”Ÿæˆä¸­æ–‡æ‘˜è¦å’Œè¡ŒåŠ¨å»ºè®®
"""

import json
import os
import sys
import hashlib
import re
import time
import random
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, List, Any, Optional
import feedparser
import requests

# â”€â”€â”€ è·¯å¾„é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SCRIPT_DIR = Path(__file__).parent
BLOG_ROOT = SCRIPT_DIR.parent
POSTS_DIR = BLOG_ROOT / "posts" / "x-signals"
SOURCES_CONFIG = BLOG_ROOT / "config" / "x-sources-categorized.json"
ENV_FILE = BLOG_ROOT / ".env.local"

# â”€â”€â”€ è¯»å–ç¯å¢ƒå˜é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_env() -> Dict[str, str]:
    """è¯»å– .env.local æ–‡ä»¶"""
    env = {}
    if ENV_FILE.exists():
        for line in ENV_FILE.read_text(encoding="utf-8").splitlines():
            line = line.strip()
            if "=" in line and not line.startswith("#"):
                k, v = line.split("=", 1)
                env[k.strip()] = v.strip()
    return env

ENV = load_env()
GEMINI_API_KEY = ENV.get("GEMINI_API_KEY", "")
GLM_API_KEY = ENV.get("GLM_API_KEY", "")

# â”€â”€â”€ Nitter å…¬å¼€å®ä¾‹åˆ—è¡¨ï¼ˆè‡ªåŠ¨è½®æ¢ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

NITTER_INSTANCES = [
    "https://nitter.net",
    "https://nitter.privacydev.net",
    "https://nitter.poast.org",
    "https://nitter.1d4.us",
    "https://nitter.kavin.rocks",
    "https://nitter.unixfox.eu",
    "https://nitter.catsarch.com",
    "https://nitter.moomoo.me",
]

# â”€â”€â”€ å»é‡å·¥å…· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_fingerprint(content: str) -> str:
    """å¯¹å†…å®¹å‰100å­—ç¬¦ç”Ÿæˆ md5 æŒ‡çº¹ï¼Œç”¨äºå»é‡"""
    clean = re.sub(r'\s+', ' ', content[:100].lower().strip())
    return hashlib.md5(clean.encode()).hexdigest()

def load_existing_dedupe_set() -> set:
    """åŠ è½½è¿‡å» 7 å¤©å·²å‘å¸ƒå†…å®¹çš„é“¾æ¥å’ŒæŒ‡çº¹"""
    seen = set()
    POSTS_DIR.mkdir(parents=True, exist_ok=True)
    files = sorted(POSTS_DIR.glob("*.md"), reverse=True)[:7]
    for f in files:
        text = f.read_text(encoding="utf-8")
        # æ”¶é›†é“¾æ¥
        for link in re.findall(r'https?://x\.com/\S+/status/\d+', text):
            seen.add(link)
        # æ”¶é›†å†…å®¹æŒ‡çº¹
        for fp in re.findall(r'<!--fp:([a-f0-9]{32})-->', text):
            seen.add(fp)
    print(f"ğŸ“š å»é‡åº“åŠ è½½å®Œæˆï¼Œå…± {len(seen)} æ¡å†å²è®°å½•")
    return seen

# â”€â”€â”€ Nitter RSS æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_user_tweets(username: str, dedupe: set, max_retries: int = 3) -> List[Dict]:
    """
    é€šè¿‡ Nitter å…¬å¼€ RSS æŠ“å–ç”¨æˆ·æœ€æ–°æ¨æ–‡ã€‚
    è‡ªåŠ¨è½®æ¢å®ä¾‹ï¼Œå¤±è´¥æ—¶é‡è¯•ã€‚
    """
    instances = NITTER_INSTANCES.copy()
    random.shuffle(instances)  # éšæœºåŒ–ï¼Œåˆ†æ•£è´Ÿè½½
    
    cutoff = datetime.now(timezone.utc) - timedelta(days=3)
    
    for instance in instances[:max_retries]:
        url = f"{instance}/{username}/rss"
        try:
            feed = feedparser.parse(url, agent="Mozilla/5.0 (compatible; RSS reader)")
            if not feed.entries:
                continue
            
            tweets = []
            for entry in feed.entries:
                # è§£æå‘å¸ƒæ—¶é—´
                pub = entry.get("published_parsed") or entry.get("updated_parsed")
                if pub:
                    pub_dt = datetime(*pub[:6], tzinfo=timezone.utc)
                    if pub_dt < cutoff:
                        continue
                
                content = entry.get("title") or entry.get("summary") or ""
                # æ¸…ç† Nitter å†…å®¹ (å»æ‰ RTã€@reply ç­‰)
                content = re.sub(r'^RT @\w+:', '', content).strip()
                content = re.sub(r'<[^>]+>', '', content).strip()
                
                if len(content) < 20:  # è·³è¿‡å¤ªçŸ­çš„å†…å®¹
                    continue
                
                link = entry.get("link", "")
                # æ ‡å‡†åŒ–é“¾æ¥ä¸º x.com æ ¼å¼
                link = re.sub(r'https?://[^/]+/', 'https://x.com/', link)
                
                fp = get_fingerprint(content)
                
                # å»é‡æ£€æŸ¥
                if link in dedupe or fp in dedupe:
                    continue
                
                tweets.append({
                    "author": f"@{username}",
                    "content": content,
                    "link": link,
                    "pubDate": entry.get("published", ""),
                    "fingerprint": fp,
                })
            
            if tweets:  # æˆåŠŸè·å–ï¼Œç›´æ¥è¿”å›
                return tweets
                
        except Exception as e:
            print(f"    âš  {instance} å¤±è´¥: {e}")
            time.sleep(0.5)
            continue
    
    return []

# â”€â”€â”€ åˆ†ç±»é…ç½®åŠ è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_categories() -> Dict[str, Any]:
    """åŠ è½½åˆ†ç±»é…ç½®"""
    if not SOURCES_CONFIG.exists():
        print(f"âŒ æ‰¾ä¸åˆ°åˆ†ç±»é…ç½®: {SOURCES_CONFIG}")
        sys.exit(1)
    return json.loads(SOURCES_CONFIG.read_text(encoding="utf-8"))

# â”€â”€â”€ AI ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def call_gemini(prompt: str) -> str:
    """è°ƒç”¨ Gemini API"""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}"
    resp = requests.post(url, json={"contents": [{"parts": [{"text": prompt}]}]}, timeout=60)
    resp.raise_for_status()
    return resp.json()["candidates"][0]["content"]["parts"][0]["text"]

def call_glm(prompt: str) -> str:
    """è°ƒç”¨ GLM-4 API"""
    resp = requests.post(
        "https://open.bigmodel.cn/api/paas/v4/chat/completions",
        headers={"Authorization": f"Bearer {GLM_API_KEY}", "Content-Type": "application/json"},
        json={"model": "glm-4-plus", "messages": [{"role": "user", "content": prompt}], "temperature": 0.7, "max_tokens": 4096},
        timeout=60
    )
    resp.raise_for_status()
    return resp.json()["choices"][0]["message"]["content"]

def call_ai(prompt: str) -> str:
    """ä¼˜å…ˆ Geminiï¼Œå›é€€ GLM-4"""
    if GEMINI_API_KEY:
        return call_gemini(prompt)
    elif GLM_API_KEY:
        return call_glm(prompt)
    else:
        raise RuntimeError("æœªæ‰¾åˆ° AI API Keyï¼ˆGEMINI_API_KEY æˆ– GLM_API_KEYï¼‰")

# â”€â”€â”€ æ¿å—ä¿¡å·ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_category_signal(category_name: str, description: str, tweets: List[Dict], date_str: str) -> Optional[Dict]:
    """ä¸ºå•ä¸ªæ¿å—ç”Ÿæˆç²¾é€‰ä¿¡å· + æ‘˜è¦ + è¡ŒåŠ¨å»ºè®®"""
    if not tweets:
        print(f"  ğŸ“­ [{category_name}] æ— å†…å®¹ï¼Œè·³è¿‡")
        return None
    
    tweet_list = "\n---\n".join([
        f"[{i+1}] {t['author']}\nå†…å®¹: {t['content'][:300]}\né“¾æ¥: {t['link']}"
        for i, t in enumerate(tweets)
    ])
    
    prompt = f"""ä½ æ˜¯"Potato"ï¼Œé¡¶çº§ä¿¡æ¯ç­–å±•äººã€‚ä»Šå¤©æ˜¯ {date_str}ã€‚

ä½ ç®¡ç†æ¿å—ã€Œ{category_name}ã€ï¼ˆä¸»é¢˜ï¼š{description}ï¼‰ã€‚

ä»¥ä¸‹æ˜¯è¯¥æ¿å—çš„æœ€æ–°æ¨æ–‡ï¼ˆå…± {len(tweets)} æ¡ï¼‰ï¼š

{tweet_list}

## ä»»åŠ¡

1. **ç²¾é€‰**ï¼šé€‰å‡ºæœ€æœ‰ä»·å€¼çš„ 5~10 æ¡ï¼ˆè‹¥å°‘äº5æ¡åˆ™å…¨é€‰ï¼‰
2. **æç‚¼**ï¼šæ¯æ¡ç”¨ä¸€å¥è¯ï¼ˆâ‰¤60å­—ä¸­æ–‡ï¼‰æç‚¼æ ¸å¿ƒæ´è§ï¼ŒçŠ€åˆ©æœ‰åŠ›
3. **æ€»ç»“**ï¼šå†™ä¸€æ®µæ¿å—æ€»ç»“ï¼ˆ100-160å­—ï¼‰ï¼Œæ­ç¤ºå…±åŒè¶‹åŠ¿
4. **è¡ŒåŠ¨**ï¼šç»™å‡º 2~3 æ¡å…·ä½“å¯æ‰§è¡Œå»ºè®®ï¼Œä»¥"ä¸»æƒæ„å»ºè€…"è§†è§’

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼Œåªè¾“å‡ºæ ¼å¼å†…å®¹ï¼Œä¸è¦å¤šä½™æ–‡å­—ï¼‰

ITEMS_START
åºå·|è´¦å·|ä¿¡å·æç‚¼ï¼ˆä¸€å¥è¯ï¼‰|é“¾æ¥
1|@è´¦å·å|ä¿¡å·å†…å®¹|https://...
ITEMS_END
SUMMARY_START
[æ¿å—æ€»ç»“]
SUMMARY_END
ACTIONS_START
- [å»ºè®®1]
- [å»ºè®®2]
- [å»ºè®®3]
ACTIONS_END"""
    
    print(f"  ğŸ¤– [{category_name}] æ­£åœ¨ç”Ÿæˆï¼ˆ{len(tweets)} æ¡è¾“å…¥ï¼‰...")
    
    try:
        raw = call_ai(prompt)
        return parse_ai_response(category_name, tweets, raw)
    except Exception as e:
        print(f"  âŒ [{category_name}] AI ç”Ÿæˆå¤±è´¥: {e}")
        return None

def parse_ai_response(category_name: str, source_tweets: List[Dict], raw: str) -> Dict:
    """è§£æ AI è¾“å‡º"""
    items = []
    
    # è§£æ ITEMS
    items_match = re.search(r'ITEMS_START\n(.*?)\nITEMS_END', raw, re.DOTALL)
    if items_match:
        for line in items_match.group(1).strip().split('\n'):
            parts = line.split('|')
            if len(parts) >= 4 and parts[0].strip().isdigit():
                author_handle = parts[1].strip().lstrip('@').lower()
                signal = parts[2].strip()
                link = parts[3].strip()
                
                # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ¨æ–‡æŒ‡çº¹
                source = next(
                    (t for t in source_tweets if 
                     t['author'].lstrip('@').lower() == author_handle or t['link'] == link),
                    None
                )
                fp = source['fingerprint'] if source else get_fingerprint(signal)
                
                items.append({
                    "author": parts[1].strip(),
                    "signal": signal,
                    "link": link,
                    "fingerprint": fp,
                })
    
    # è§£æ SUMMARY
    summary_match = re.search(r'SUMMARY_START\n(.*?)\nSUMMARY_END', raw, re.DOTALL)
    summary = summary_match.group(1).strip() if summary_match else "æš‚æ— æ€»ç»“ã€‚"
    
    # è§£æ ACTIONS
    actions_match = re.search(r'ACTIONS_START\n(.*?)\nACTIONS_END', raw, re.DOTALL)
    actions = []
    if actions_match:
        actions = [
            line.lstrip('- ').strip()
            for line in actions_match.group(1).strip().split('\n')
            if line.strip()
        ]
    
    return {
        "category_name": category_name,
        "items": items,
        "summary": summary,
        "actions": actions,
    }

# â”€â”€â”€ Markdown æ¸²æŸ“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def render_category_section(signal: Dict) -> str:
    rows = "\n".join([
        f"| {i+1} | {item['author']} | {item['signal']} <!--fp:{item['fingerprint']}--> | [â†’ åŸæ–‡]({item['link']}) |"
        for i, item in enumerate(signal['items'])
    ])
    action_list = "\n".join([f"- ğŸ’¡ {a}" for a in signal['actions']])
    
    return f"""## {signal['category_name']}

| # | æ¥æº | ä»Šæ—¥ä¿¡å· | é“¾æ¥ |
|---|------|---------|------|
{rows}

> **æ¿å—æ€»ç»“** | {signal['summary']}

**âš¡ è¡ŒåŠ¨å»ºè®®**
{action_list}

"""

# â”€â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    print("â•" * 50)
    print("  ğŸ›°  X Signal Categorized Briefing (Python)")
    print("â•" * 50)
    
    # æ—¥æœŸè®¾ç½®
    now = datetime.now()
    date_str = now.strftime("%Y-%m-%d")
    filename = f"{date_str}-daily-signals.md"
    filepath = POSTS_DIR / filename
    POSTS_DIR.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½é…ç½®å’Œå»é‡
    config = load_categories()
    dedupe = load_existing_dedupe_set()
    
    # æŒ‰æ¿å—æŠ“å–æ¨æ–‡
    category_tweets: Dict[str, List[Dict]] = {}
    total_fetched = 0
    
    for cat_name, cat_config in config["categories"].items():
        print(f"\nğŸ“¡ æŠ“å– [{cat_name}]...")
        cat_tweets = []
        
        for username in cat_config["accounts"]:
            tweets = fetch_user_tweets(username, dedupe)
            if tweets:
                print(f"    âœ… @{username}: {len(tweets)} æ¡")
                cat_tweets.extend(tweets)
            time.sleep(random.uniform(0.3, 0.8))  # ç¤¼è²Œå»¶è¿Ÿï¼Œé¿å…è§¦å‘é™æµ
        
        # æŒ‰æ—¶é—´æ’åºï¼Œå–æœ€æ–° 25 æ¡é€ AI
        cat_tweets.sort(key=lambda t: t.get("pubDate", ""), reverse=True)
        category_tweets[cat_name] = cat_tweets[:25]
        total_fetched += len(cat_tweets)
        print(f"  ğŸ“Š [{cat_name}] å…± {len(cat_tweets)} æ¡æ–°å†…å®¹")
    
    print(f"\nğŸ“Š æ€»è®¡æŠ“å–: {total_fetched} æ¡æ–°æ¨æ–‡")
    
    if total_fetched == 0:
        print("\nâš ï¸  æ‰€æœ‰ Nitter å®ä¾‹å‡æ— æ³•è®¿é—®æˆ–æ— æ–°å†…å®¹ã€‚")
        print("ğŸ’¡ å¯èƒ½åŸå› ï¼š")
        print("   1. Nitter å®ä¾‹è¢«é™æµï¼Œè¯·ç¨åé‡è¯•")
        print("   2. è®¢é˜…è´¦å·è¿‘3å¤©æ— æ–°æ¨æ–‡")
        return
    
    # ç”Ÿæˆæ¿å—ä¿¡å·
    print("\nğŸ¤– å¼€å§‹ç”Ÿæˆ AI åˆ†æ...\n")
    all_signals = []
    selected_fps = set()
    
    for cat_name, tweets in category_tweets.items():
        desc = config["categories"][cat_name]["description"]
        signal = generate_category_signal(cat_name, desc, tweets, date_str)
        if signal and signal["items"]:
            all_signals.append(signal)
            # æ›´æ–°å»é‡é›†åˆï¼ˆæœ¬æ¬¡è¿è¡Œå†…ä¹Ÿå»é‡ï¼‰
            for item in signal["items"]:
                selected_fps.add(item["fingerprint"])
                selected_fps.add(item["link"])
                dedupe.add(item["fingerprint"])
                dedupe.add(item["link"])
    
    if not all_signals:
        print("âŒ æœªèƒ½ç”Ÿæˆä»»ä½•æ¿å—ä¿¡å·ï¼Œè¯·æ£€æŸ¥ AI API Key")
        return
    
    # ç»Ÿè®¡
    total_items = sum(len(s["items"]) for s in all_signals)
    
    # ç”Ÿæˆ Markdown
    category_sections = "---\n\n".join([render_category_section(s) for s in all_signals])
    
    full_content = f"""---
title: "X Signal Daily Briefing: {date_str}"
date: "{date_str}"
category: "X Signal"
tags: ["X", "AI", "è´¢å¯Œ", "è¥é”€", "æ™ºæ…§", "æ—¥æŠ¥"]
---

> **Potato's Daily Briefing** Â· {date_str} Â· ä»Šæ—¥ç²¾é€‰ **{total_items}** æ¡ä¿¡å·ï¼Œè¦†ç›– **{len(all_signals)}** ä¸ªæ¿å—

---

{category_sections}---

*Curated by Potato Â· Powered by Python + Nitter Â· {now.strftime("%H:%M")}*
"""
    
    # å†™å…¥æ–‡ä»¶
    if filepath.exists():
        # è¿½åŠ æ›´æ–°
        existing = filepath.read_text(encoding="utf-8")
        update_block = f"\n\n## ğŸ”„ æ›´æ–° {now.strftime('%H:%M')}\n\n{category_sections}"
        filepath.write_text(existing + update_block, encoding="utf-8")
        print(f"\nâœ… å·²è¿½åŠ æ›´æ–°åˆ°: {filename}")
    else:
        filepath.write_text(full_content, encoding="utf-8")
        print(f"\nâœ… å·²åˆ›å»ºæ–°æ—¥æŠ¥: {filename}")
    
    # æ‰“å°ç»Ÿè®¡
    print(f"\nğŸ“‹ ç”Ÿæˆç»Ÿè®¡:")
    for s in all_signals:
        print(f"  {s['category_name']}: {len(s['items'])} æ¡ä¿¡å·")
    print("â•" * 50)
    print(f"  æ–‡ä»¶è·¯å¾„: {filepath}")
    print("â•" * 50 + "\n")


if __name__ == "__main__":
    main()
