#!/usr/bin/env python3
"""
AI News Scraper - æŠ“å– AI åº”ç”¨å’Œè¥é”€èµ„è®¯çš„å·¥å…·
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

import feedparser
import requests
from bs4 import BeautifulSoup
import yaml

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = Path(__file__).parent / "config.yaml"
DATA_DIR = Path(__file__).parent / "data"


class AINewsScraper:
    """AI æ–°é—»æŠ“å–å™¨"""

    def __init__(self, config_file: str = None):
        """åˆå§‹åŒ–æŠ“å–å™¨"""
        self.config = self._load_config(config_file)
        self.data_dir = DATA_DIR
        self.data_dir.mkdir(exist_ok=True)
        self.all_articles = []

    def _load_config(self, config_file: str = None) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = Path(config_file) if config_file else CONFIG_FILE
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        return {}

    def scrape_rss(self) -> List[Dict[str, Any]]:
        """æŠ“å– RSS æº"""
        articles = []
        rss_sources = self.config.get('rss_sources', [])

        print(f"\nğŸ“¡ å¼€å§‹æŠ“å– {len(rss_sources)} ä¸ª RSS æº...")

        for source in rss_sources:
            name = source.get('name', 'Unknown')
            url = source.get('url')
            if not url:
                continue

            try:
                print(f"  æŠ“å–: {name}")
                feed = feedparser.parse(url)

                for entry in feed.entries[:self.config.get('max_articles_per_feed', 10)]:
                    article = {
                        'title': entry.get('title', ''),
                        'link': entry.get('link', ''),
                        'summary': self._clean_html(entry.get('summary', entry.get('description', ''))),
                        'published': self._parse_date(entry.get('published', '')),
                        'source': name,
                        'source_type': 'RSS',
                        'scraped_at': datetime.now().isoformat()
                    }
                    articles.append(article)
            except Exception as e:
                print(f"  âŒ {name} æŠ“å–å¤±è´¥: {e}")

        return articles

    def scrape_reddit(self) -> List[Dict[str, Any]]:
        """æŠ“å– Reddit"""
        articles = []
        subreddits = self.config.get('reddit_subreddits', [])

        if not subreddits:
            return articles

        print(f"\nğŸ‘¾ å¼€å§‹æŠ“å– Reddit...")

        # Reddit å…¬å¼€ RSS ä¸éœ€è¦ API key
        for subreddit in subreddits:
            try:
                url = f"https://www.reddit.com/r/{subreddit}/hot.rss"
                print(f"  æŠ“å–: r/{subreddit}")
                feed = feedparser.parse(url)

                for entry in feed.entries[:self.config.get('max_articles_per_feed', 10)]:
                    article = {
                        'title': entry.get('title', ''),
                        'link': entry.get('link', ''),
                        'summary': self._clean_html(entry.get('summary', '')),
                        'published': self._parse_date(entry.get('published', '')),
                        'source': f'reddit/r/{subreddit}',
                        'source_type': 'Reddit',
                        'scraped_at': datetime.now().isoformat()
                    }
                    articles.append(article)
            except Exception as e:
                print(f"  âŒ r/{subreddit} æŠ“å–å¤±è´¥: {e}")

        return articles

    def _clean_html(self, html: str) -> str:
        """æ¸…ç† HTML æ ‡ç­¾"""
        if not html:
            return ""
        soup = BeautifulSoup(html, 'html.parser')
        return soup.get_text(strip=True)[:500]

    def _parse_date(self, date_str: str) -> str:
        """è§£ææ—¥æœŸ"""
        try:
            dt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %z')
            return dt.isoformat()
        except:
            return date_str

    def run(self):
        """è¿è¡ŒæŠ“å–"""
        print("ğŸš€ AI News Scraper å¯åŠ¨...")
        print(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # æŠ“å–å„ä¸ªæº
        self.all_articles.extend(self.scrape_rss())
        self.all_articles.extend(self.scrape_reddit())

        # å»é‡
        seen = set()
        unique_articles = []
        for article in self.all_articles:
            key = article['title'] + article['link']
            if key not in seen:
                seen.add(key)
                unique_articles.append(article)

        self.all_articles = unique_articles

        print(f"\nâœ… å…±æŠ“å–åˆ° {len(self.all_articles)} æ¡èµ„è®¯")

        # ä¿å­˜æ•°æ®
        self._save_json()
        self._save_markdown()

    def _save_json(self):
        """ä¿å­˜ä¸º JSON"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = self.data_dir / f"ai_news_{timestamp}.json"

        data = {
            'scraped_at': datetime.now().isoformat(),
            'total_articles': len(self.all_articles),
            'articles': self.all_articles
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        # åŒæ—¶ä¿å­˜ä¸º latest.json
        latest_file = self.data_dir / "latest.json"
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ JSON å·²ä¿å­˜: {filename}")

    def _save_markdown(self):
        """ä¿å­˜ä¸º Markdown"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        filename = self.data_dir / f"ai_news_{datetime.now().strftime('%Y%m%d')}.md"

        # æŒ‰æ¥æºåˆ†ç»„
        by_source = {}
        for article in self.all_articles:
            source = article['source']
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(article)

        # ç”Ÿæˆ Markdown
        content = f"# AI èµ„è®¯æ—¥æŠ¥\n\n"
        content += f"**æŠ“å–æ—¶é—´**: {timestamp}\n"
        content += f"**æ–‡ç« æ•°é‡**: {len(self.all_articles)} ç¯‡\n\n---\n\n"

        for source, articles in by_source.items():
            content += f"## {source}\n\n"
            for article in articles:
                content += f"### {article['title']}\n\n"
                if article['summary']:
                    content += f"**æ‘˜è¦**: {article['summary']}\n\n"
                content += f"- **é“¾æ¥**: [{article['link']}]({article['link']})\n"
                content += f"- **æ—¶é—´**: {article['published']}\n\n"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)

        # åŒæ—¶ä¿å­˜ä¸º daily.md
        daily_file = self.data_dir / "daily.md"
        with open(daily_file, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"ğŸ“„ Markdown å·²ä¿å­˜: {filename}")


def main():
    """ä¸»å‡½æ•°"""
    scraper = AINewsScraper()
    scraper.run()


if __name__ == '__main__':
    main()
